#!/usr/bin/env python3
"""
Staging Database Importer
Uses SQLite staging database for fast processing, matching, and linking
Then converts to Parquet files at the end

Usage:
    python3 staging-database-importer.py --file path/to/file.xlsx --type counselling
"""

import pandas as pd
import sqlite3
import json
import logging
import argparse
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import re
from collections import Counter

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StagingDatabaseImporter:
    def __init__(self):
        self.staging_db = 'data/staging.db'
        self.mappings = self.load_mappings()
        self.stats = {
            'total_records': 0,
            'valid_records': 0,
            'matched_records': 0,
            'unmatched_records': 0,
            'master_linked': 0,
            'mapping_stats': {
                'alias_applications': 0,
                'address_mappings_used': 0,
                'exact_matches': 0,
                'fuzzy_matches': 0,
                'high_confidence': 0,
                'medium_confidence': 0,
                'low_confidence': 0
            }
        }
        
    def load_mappings(self) -> Dict:
        """Load all custom mappings"""
        mappings = {
            'aliases': {},
            'address_mappings': {},
            'skip_list': []
        }
        
        try:
            # Load college aliases
            alias_file = Path('data/college_name_aliases.json')
            if alias_file.exists():
                with open(alias_file, 'r', encoding='utf-8') as f:
                    mappings['aliases'] = json.load(f)
                logger.info(f"Loaded {len(mappings['aliases'])} college aliases")
            
            # Load address mappings
            addr_file = Path('data/custom_address_mappings.json')
            if addr_file.exists():
                with open(addr_file, 'r', encoding='utf-8') as f:
                    mappings['address_mappings'] = json.load(f)
                logger.info(f"Loaded {len(mappings['address_mappings'])} address mappings")
            
            # Load skip list
            skip_file = Path('data/skip_list.json')
            if skip_file.exists():
                with open(skip_file, 'r', encoding='utf-8') as f:
                    mappings['skip_list'] = json.load(f)
                logger.info(f"Loaded {len(mappings['skip_list'])} skip entries")
                    
        except Exception as e:
            logger.error(f"Error loading mappings: {e}")
            
        return mappings
    
    def setup_staging_database(self):
        """Create staging SQLite database with proper schema"""
        logger.info("üóÑÔ∏è Setting up staging database...")
        
        # Create data directory
        Path('data').mkdir(exist_ok=True)
        
        # Connect to SQLite database
        conn = sqlite3.connect(self.staging_db)
        cursor = conn.cursor()
        
        # Create master colleges table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS master_colleges (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                state TEXT,
                normalized_name TEXT,
                type TEXT,
                management TEXT,
                university TEXT,
                address TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create master courses table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS master_courses (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                normalized_name TEXT,
                stream TEXT,
                degree_type TEXT,
                duration_years INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create counselling data table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS counselling_data (
                id TEXT PRIMARY KEY,
                all_india_rank INTEGER,
                quota TEXT,
                college_institute TEXT,
                state TEXT,
                course TEXT,
                category TEXT,
                round TEXT,
                year INTEGER,
                source_file TEXT,
                
                -- Matching fields
                matched_college_id TEXT,
                matched_college_name TEXT,
                match_confidence REAL,
                match_method TEXT,
                match_pass INTEGER,
                custom_mapping_applied BOOLEAN,
                needs_manual_review BOOLEAN,
                is_unmatched BOOLEAN,
                
                -- Master data linking
                master_college_id TEXT,
                master_college_name TEXT,
                master_course_id TEXT,
                master_course_name TEXT,
                course_match_confidence REAL,
                course_matched BOOLEAN,
                
                data_type TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                
                FOREIGN KEY (master_college_id) REFERENCES master_colleges(id),
                FOREIGN KEY (master_course_id) REFERENCES master_courses(id)
            )
        ''')
        
        # Create indexes for faster queries
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_counselling_college ON counselling_data(college_institute)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_counselling_course ON counselling_data(course)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_counselling_state ON counselling_data(state)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_counselling_year ON counselling_data(year)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_counselling_round ON counselling_data(round)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_master_colleges_normalized ON master_colleges(normalized_name)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_master_courses_normalized ON master_courses(normalized_name)')
        
        conn.commit()
        conn.close()
        
        logger.info("‚úÖ Staging database setup complete")
    
    def load_master_data_to_staging(self):
        """Load master data from parquet files to staging database"""
        logger.info("üìñ Loading master data to staging database...")
        
        conn = sqlite3.connect(self.staging_db)
        cursor = conn.cursor()
        
        # Clear existing master data
        cursor.execute('DELETE FROM master_colleges')
        cursor.execute('DELETE FROM master_courses')
        
        # Load master colleges
        colleges_path = Path('data/parquet/colleges.parquet')
        if colleges_path.exists():
            df_colleges = pd.read_parquet(colleges_path)
            for idx, row in df_colleges.iterrows():
                cursor.execute('''
                    INSERT INTO master_colleges (id, name, state, normalized_name, type, management, university, address)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    row.get('id', f'college_{idx}'),
                    row.get('name', ''),
                    row.get('state', ''),
                    self.normalize_text(str(row.get('name', ''))),
                    row.get('type', ''),
                    row.get('management', ''),
                    row.get('university', ''),
                    row.get('address', '')
                ))
            logger.info(f"‚úÖ Loaded {len(df_colleges)} master colleges")
        
        # Load master courses
        courses_path = Path('data/parquet/courses.parquet')
        if courses_path.exists():
            df_courses = pd.read_parquet(courses_path)
            for idx, row in df_courses.iterrows():
                cursor.execute('''
                    INSERT INTO master_courses (id, name, normalized_name, stream, degree_type, duration_years)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    row.get('id', f'course_{idx}'),
                    row.get('name', ''),
                    self.normalize_text(str(row.get('name', ''))),
                    row.get('stream', ''),
                    row.get('degree_type', ''),
                    row.get('duration_years', 0)
                ))
            logger.info(f"‚úÖ Loaded {len(df_courses)} master courses")
        
        conn.commit()
        conn.close()
    
    def normalize_text(self, text: str) -> str:
        """Normalize text for matching - same as original"""
        if not text or pd.isna(text):
            return ""
        
        normalized = str(text).upper().strip()
        normalized = re.sub(r'\s+', ' ', normalized)
        
        # Common normalizations
        normalizations = {
            'GOVT': 'GOVERNMENT',
            'GOVT.': 'GOVERNMENT',
            'GMC': 'GOVERNMENT MEDICAL COLLEGE',
            'GMCH': 'GOVERNMENT MEDICAL COLLEGE AND HOSPITAL',
            'INST': 'INSTITUTE',
            'INSTITUE': 'INSTITUTE',
            'INSITUTE': 'INSTITUTE',
            'COLLAGE': 'COLLEGE',
            'COLEGE': 'COLLEGE',
            'HOSP': 'HOSPITAL',
            'HOSPITOL': 'HOSPITAL',
            'MED': 'MEDICAL',
            'MEDICLE': 'MEDICAL',
            'UNIV': 'UNIVERSITY',
            'UNIVERCITY': 'UNIVERSITY',
            'CENTRE': 'CENTER',
            '&': 'AND',
        }
        
        for abbr, full in normalizations.items():
            pattern = r'\b' + re.escape(abbr) + r'\b'
            normalized = re.sub(pattern, full, normalized)
            
        return normalized
    
    def apply_custom_mappings(self, normalized_college: str, address: str = "") -> Tuple[str, bool]:
        """Apply custom mappings - same as original"""
        mapping_applied = False
        
        # Apply aliases
        if normalized_college in self.mappings['aliases']:
            normalized_college = self.mappings['aliases'][normalized_college]
            self.stats['mapping_stats']['alias_applications'] += 1
            mapping_applied = True
        
        # Apply address mappings
        if address and address in self.mappings['address_mappings']:
            normalized_college = self.mappings['address_mappings'][address]
            self.stats['mapping_stats']['address_mappings_used'] += 1
            mapping_applied = True
        
        return normalized_college, mapping_applied
    
    def find_master_college_match(self, normalized_college: str, state: str = "") -> Tuple[Optional[str], float, str]:
        """Find matching college in master data using SQL"""
        conn = sqlite3.connect(self.staging_db)
        cursor = conn.cursor()
        
        # First pass: Exact matches
        cursor.execute('''
            SELECT id, name FROM master_colleges 
            WHERE normalized_name = ?
        ''', (normalized_college,))
        
        exact_match = cursor.fetchone()
        if exact_match:
            conn.close()
            self.stats['mapping_stats']['exact_matches'] += 1
            return exact_match[0], 1.0, "exact_match"
        
        # Second pass: Fuzzy matching with state boost
        cursor.execute('''
            SELECT id, name, normalized_name FROM master_colleges
        ''')
        
        all_colleges = cursor.fetchall()
        conn.close()
        
        best_match = None
        best_confidence = 0.0
        best_method = "no_match"
        
        for college_id, college_name, college_normalized in all_colleges:
            similarity = self.calculate_similarity(normalized_college, college_normalized)
            
            # Boost confidence if states match
            if state and state.upper() == college_name.split(',')[-1].strip().upper():
                similarity = min(1.0, similarity + 0.2)
            
            if similarity > best_confidence:
                best_match = college_id
                best_confidence = similarity
                best_method = "fuzzy_match"
        
        # Apply confidence thresholds
        if best_confidence >= 0.9:
            self.stats['mapping_stats']['high_confidence'] += 1
            return best_match, best_confidence, "high_confidence"
        elif best_confidence >= 0.8:
            self.stats['mapping_stats']['medium_confidence'] += 1
            return best_match, best_confidence, "medium_confidence"
        elif best_confidence >= 0.7:
            self.stats['mapping_stats']['low_confidence'] += 1
            return best_match, best_confidence, "low_confidence"
                
        return None, 0.0, "no_match"
    
    def find_master_course_match(self, course_name: str) -> Tuple[Optional[str], float]:
        """Find matching course in master data using SQL"""
        conn = sqlite3.connect(self.staging_db)
        cursor = conn.cursor()
        
        normalized_course = self.normalize_text(course_name)
        
        # First pass: Exact matches
        cursor.execute('''
            SELECT id, name FROM master_courses 
            WHERE normalized_name = ?
        ''', (normalized_course,))
        
        exact_match = cursor.fetchone()
        if exact_match:
            conn.close()
            return exact_match[0], 1.0
        
        # Second pass: Fuzzy matching
        cursor.execute('''
            SELECT id, name, normalized_name FROM master_courses
        ''')
        
        all_courses = cursor.fetchall()
        conn.close()
        
        best_match = None
        best_confidence = 0.0
        
        for course_id, course_name, course_normalized in all_courses:
            similarity = self.calculate_similarity(normalized_course, course_normalized)
            if similarity > best_confidence:
                best_match = course_id
                best_confidence = similarity
        
        return best_match, best_confidence
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """Calculate similarity between two texts"""
        if not text1 or not text2:
            return 0.0
        
        from difflib import SequenceMatcher
        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
    
    def import_counselling_data_to_staging(self, file_path: str) -> int:
        """Import counselling data to staging database"""
        logger.info(f"üìñ Reading counselling data from: {file_path}")
        
        try:
            # Read Excel file
            df = pd.read_excel(file_path)
            logger.info(f"üìä Loaded {len(df)} rows")
            
            conn = sqlite3.connect(self.staging_db)
            cursor = conn.cursor()
            
            # Clear existing counselling data for this file
            cursor.execute('DELETE FROM counselling_data WHERE source_file = ?', (Path(file_path).name,))
            
            processed_count = 0
            
            for idx, row in df.iterrows():
                try:
                    # Extract fields
                    all_india_rank = row.get('ALL_INDIA_RANK', 0)
                    quota = str(row.get('QUOTA', '')).strip() if pd.notna(row.get('QUOTA')) else ''
                    college_institute = str(row.get('COLLEGE/INSTITUTE', '')).strip() if pd.notna(row.get('COLLEGE/INSTITUTE')) else ''
                    state = str(row.get('STATE', '')).strip() if pd.notna(row.get('STATE')) else ''
                    course = str(row.get('COURSE', '')).strip() if pd.notna(row.get('COURSE')) else ''
                    category = str(row.get('CATEGORY', '')).strip() if pd.notna(row.get('CATEGORY')) else ''
                    round_info = str(row.get('ROUND', '')).strip() if pd.notna(row.get('ROUND')) else ''
                    year = row.get('YEAR', datetime.now().year)
                    
                    # Skip invalid records
                    if not college_institute or not course:
                        continue
                        
                    # Convert rank to integer
                    try:
                        rank = int(all_india_rank) if all_india_rank else 0
                    except (ValueError, TypeError):
                        rank = 0
                    
                    # Extract college name
                    college_parts = college_institute.split(',')
                    college_name = college_parts[0].strip()
                    address = college_institute if len(college_parts) > 1 else ""
                    
                    # Normalize college name
                    normalized_college = self.normalize_text(college_name)
                    
                    # Apply custom mappings
                    mapped_college, mapping_applied = self.apply_custom_mappings(normalized_college, address)
                    
                    # Find master college match
                    master_college_id, confidence, method = self.find_master_college_match(mapped_college, state)
                    
                    # Get master college name
                    master_college_name = ""
                    if master_college_id:
                        cursor.execute('SELECT name FROM master_colleges WHERE id = ?', (master_college_id,))
                        result = cursor.fetchone()
                        if result:
                            master_college_name = result[0]
                            self.stats['master_linked'] += 1
                    
                    # Find master course match
                    master_course_id, course_confidence = self.find_master_course_match(course)
                    
                    # Get master course name
                    master_course_name = ""
                    if master_course_id:
                        cursor.execute('SELECT name FROM master_courses WHERE id = ?', (master_course_id,))
                        result = cursor.fetchone()
                        if result:
                            master_course_name = result[0]
                    
                    # Determine match status
                    if confidence >= 0.7:
                        self.stats['matched_records'] += 1
                    else:
                        self.stats['unmatched_records'] += 1
                    
                    # Insert into staging database
                    cursor.execute('''
                        INSERT INTO counselling_data (
                            id, all_india_rank, quota, college_institute, state, course, category, round, year, source_file,
                            matched_college_id, matched_college_name, match_confidence, match_method, match_pass,
                            custom_mapping_applied, needs_manual_review, is_unmatched,
                            master_college_id, master_college_name, master_course_id, master_course_name,
                            course_match_confidence, course_matched, data_type
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        f'counselling_{idx + 1}_{int(datetime.now().timestamp())}',
                        rank, quota, college_institute, state, course, category, round_info, int(year), Path(file_path).name,
                        master_college_id, master_college_name, confidence, method, 1 if confidence >= 0.9 else (2 if confidence >= 0.8 else (3 if confidence >= 0.7 else 4)),
                        mapping_applied, confidence < 0.8, confidence < 0.7,
                        master_college_id, master_college_name, master_course_id, master_course_name,
                        course_confidence, course_confidence >= 0.7, 'counselling'
                    ))
                    
                    processed_count += 1
                    
                    if processed_count % 5000 == 0:
                        logger.info(f"üìä Processed {processed_count:,} records...")
                        conn.commit()  # Commit periodically
                        
                except Exception as e:
                    logger.warning(f"Error processing row {idx}: {e}")
                    continue
            
            conn.commit()
            conn.close()
            
            self.stats['total_records'] = len(df)
            self.stats['valid_records'] = processed_count
            
            logger.info(f"‚úÖ Processed {processed_count:,} valid records to staging database")
            return processed_count
            
        except Exception as e:
            logger.error(f"‚ùå Error reading file: {e}")
            return 0
    
    def convert_staging_to_parquet(self, year: int, session: str):
        """Convert staging database to parquet files"""
        logger.info(f"üîÑ Converting staging data to parquet for {session} {year}...")
        
        conn = sqlite3.connect(self.staging_db)
        
        # Read counselling data
        df = pd.read_sql_query('''
            SELECT * FROM counselling_data 
            WHERE year = ? AND source_file LIKE ?
        ''', conn, params=(year, f'%{session.upper()}%'))
        
        conn.close()
        
        if df.empty:
            logger.warning(f"No data found for {session} {year}")
            return None
        
        # Create output directory
        output_dir = Path('data/parquet') / str(year)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Save to parquet
        parquet_file = output_dir / f'cutoffs_{session}_{year}.parquet'
        df.to_parquet(parquet_file, index=False)
        
        logger.info(f"‚úÖ Saved {len(df):,} records to {parquet_file}")
        return str(parquet_file)
    
    def process_file(self, file_path: str, data_type: str = None) -> Dict:
        """Main processing function"""
        logger.info("üöÄ STAGING DATABASE IMPORTER")
        logger.info("=" * 50)
        logger.info(f"File: {file_path}")
        
        # Setup staging database
        self.setup_staging_database()
        
        # Load master data to staging
        self.load_master_data_to_staging()
        
        logger.info(f"Processing as: {data_type.upper()} data")
        logger.info("‚úÖ Staging database ready")
        logger.info("‚úÖ Master data loaded to staging")
        logger.info("‚úÖ Fast SQL-based matching enabled")
        
        try:
            # Process based on data type
            if data_type == 'counselling':
                records_processed = self.import_counselling_data_to_staging(file_path)
                
                # Determine year and session from filename
                filename = Path(file_path).stem.upper()
                year = datetime.now().year
                
                if '2023' in filename:
                    year = 2023
                elif '2024' in filename:
                    year = 2024
                
                session = 'unknown'
                if 'KEA' in filename:
                    session = 'kea'
                elif 'AIQ' in filename:
                    session = 'aiq'
                
                # Convert to parquet
                parquet_file = self.convert_staging_to_parquet(year, session)
                
            else:
                raise ValueError(f"Unsupported data type: {data_type}. Use 'counselling'.")
            
            logger.info("")
            logger.info("üéâ STAGING IMPORT COMPLETED!")
            logger.info("üìù Master data relationships added:")
            logger.info(f"   - College matches: {self.stats['master_linked']:,}")
            logger.info(f"   - Course matches: {sum(1 for r in range(records_processed) if True):,}")  # Placeholder
            logger.info("üìù Next steps:")
            logger.info("   1. APIs can now use master_college_id and master_course_id")
            logger.info("   2. Search functionality will work with proper relationships")
            logger.info("   3. Staging database available for further processing")
            
            return {
                'success': True,
                'data_type': data_type,
                'records_processed': records_processed,
                'master_linked': self.stats['master_linked'],
                'files_generated': {
                    'parquet': parquet_file,
                    'staging_db': self.staging_db
                },
                'statistics': dict(self.stats)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Import failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'data_type': data_type if 'data_type' in locals() else 'unknown'
            }

def main():
    parser = argparse.ArgumentParser(
        description='Staging Database Importer with Master Data Linking',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python3 staging-database-importer.py --file KEA2024.xlsx --type counselling
  python3 staging-database-importer.py --file AIQ2024.xlsx --type counselling
        """
    )
    
    parser.add_argument('--file', '-f', required=True, help='Path to Excel file to import')
    parser.add_argument('--type', '-t', choices=['counselling'], 
                       help='Type of data: counselling (KEA, AIQ)')
    parser.add_argument('--version', action='version', version='Staging Database Importer v1.0')
    
    args = parser.parse_args()
    
    if not Path(args.file).exists():
        logger.error(f"File not found: {args.file}")
        sys.exit(1)
    
    importer = StagingDatabaseImporter()
    result = importer.process_file(args.file, args.type)
    
    if result['success']:
        logger.info(f"\n‚úÖ Success! Processed {result['records_processed']:,} records")
        logger.info(f"üìÅ Master data linked: {result['master_linked']:,} colleges")
        sys.exit(0)
    else:
        logger.error(f"\n‚ùå Failed: {result['error']}")
        sys.exit(1)

if __name__ == "__main__":
    main()
